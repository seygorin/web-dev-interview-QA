---
title: Big O
---

## What is Big-O Notation?

Big O нотация служит стандартизированной мерой производительности алгоритмов, фокусируясь на временной и пространственной сложности. Она крайне важна для сравнения алгоритмов и понимания их масштабируемости.

### Key Concepts

- **Доминирующий член**: Нотация упрощает выражения сложности до их наиболее значимых членов, делая их легче для сравнения.

- **Асимптотический анализ**: Big O подчеркивает, как алгоритмы работают при увеличении объема данных, предоставляя высокоуровневое понимание эффективности.

- **Производительность в худшем случае**: Big O предоставляет верхний предел необходимых ресурсов, предлагая консервативную оценку для наиболее сложных сценариев.

### Visual Representation

![График сложности Big O](https://camo.githubusercontent.com/9c8f59f9d3899756b980ad9a4bd09dfa58f7d48043db623c688a94835b66a23f/68747470733a2f2f666972656261736573746f726167652e676f6f676c65617069732e636f6d2f76302f622f6465762d737461636b2d6170702e61707073706f742e636f6d2f6f2f6269672d6f2532466269672d6f2d636f6d706c65786974792532302831292e6a7065673f616c743d6d6564696126746f6b656e3d66626232376432622d333162662d343435362d386639332d663462363262306535333434)

### Complexity Classes

- **Константная сложность O(1)**: Используемые ресурсы не зависят от размера входных данных.

  - Пример по времени: Арифметические операции, доступ к элементу массива по индексу
  - Пример по памяти: Использование массива фиксированного размера

- **Логарифмическая сложность O(log n)**: Использование ресурсов растет логарифмически с размером входных данных.

  - Пример по времени: Бинарный поиск в отсортированном массиве
  - Пример по памяти: Обход бинарного дерева

- **Линейная сложность O(n)**: Использование ресурсов масштабируется линейно с размером входных данных.

  - Пример по времени: Поиск элемента в неупорядоченном списке
  - Пример по памяти: Выделение массива пропорционально размеру входных данных

- **Линейно-логарифмическая сложность O(n log n)**: Использование ресурсов растет со скоростью между линейной и квадратичной. Часто встречается при комбинировании линейных и логарифмических операций.

  - Пример по времени: Эффективные алгоритмы сортировки, такие как сортировка слиянием и быстрая сортировка
  - Пример по памяти: Алгоритмы "разделяй и властвуй", которые разбивают задачу

- **Квадратичная сложность O(n^2)**: Ресурсы масштабируются с квадратом размера входных данных.

  - Пример по времени: Алгоритмы с простыми вложенными циклами, например, сортировка пузырьком
  - Пример по памяти: Создание двумерной матрицы на основе размера входных данных

- **Экспоненциальная сложность O(2^n)**: Использование ресурсов удваивается (или увеличивается экспоненциально) с каждой дополнительной единицей входных данных.

  - Пример по времени: Генерация всех подмножеств множества
  - Пример по памяти: Создание двумерной матрицы на основе размера входных данных

- **Экспоненциальная сложность O(2^n)**: Использование ресурсов удваивается (или увеличивается экспоненциально) с каждой дополнительной единицей входных данных.

  - Пример по времени: Генерация всех подмножеств множества
  - Пример по памяти: Рекурсивные алгоритмы, которые удваивают размер стека вызовов для каждого элемента входных данных

### Practical Applications

- **Управление ресурсами**: Помогает в предварительном выделении достаточных ресурсов, особенно в ограниченных средах.

- **Надежность**: Обеспечивает гарантию производительности, что критически важно в задачах, чувствительных ко времени.

- **Оптимизация**: Помогает в выявлении узких мест и областей для потенциального улучшения, обеспечивая максимальную эффективность алгоритма.

### Code Example: Linear Search

```javascript
function linearSearch(arr, target) {
  for (let i = 0; i < arr.length; i++) {
    if (arr[i] === target) {
      return i
    }
  }

  return -1
}
```

- Худший случай: Цель не находится в списке, что приводит к временной сложности O(n).
- Лучший случай: Цель является первым элементом, с временной сложностью O(1).
- Средний случай: Упрощается до O(n), так как каждый элемент имеет равные шансы быть целью.

## Explain the difference between _Big-O_, _Big-Theta_, and _Big-Omega_ notations

Давайте обсудим значения и практическое применение трех важных нотаций:

### Big-O Notation: Upper Bound

Big-O определяет сценарий наихудшего случая производительности алгоритма. Она дает верхний предел, обозначаемый как O(g(n)), для порядка роста функции.

`0 ≤ f(n) ≤ k⋅g(n), ∃n0, k > 0`

Проще говоря, если алгоритм имеет временную сложность O(n^2), это означает, что время выполнения в худшем случае будет расти не быстрее, чем n^2.

### Big-Theta Notation: Tight Bound

Big-Theta представляет точную скорость роста, показывая как верхний, так и нижний предел для функции. Она обозначается как Θ(g(n)).

`0 ≤ k1⋅g(n) ≤ f(n) ≤ k2⋅g(n), ∃n0, k1, k2 > 0`

Алгоритм с временной сложностью, скажем, n^2, будет иметь производительность в худшем случае, которая растет как n^2 и не хуже, чем n^2.

### Big-Omega Notation: Lower Bound

Big-Omega предоставляет временную сложность лучшего случая, давая нижний предел для скорости роста функции. Она обозначается как Ω(g(n)).

0 ≤ k⋅g(n) ≤ f(n), ∃n0, k > 0

Если лучший случай временной сложности алгоритма - k⋅n, это означает, что производительность в лучшем случае будет расти как минимум как k⋅n.

### Use Case

Алгоритм, требующий определенной временной границы для эффективности, может быть подходяще описан с использованием нотации Big-Theta. Напротив, алгоритмы, разработанные для различных случаев использования, такие как адаптивные алгоритмы сортировки, лучше описываются с помощью нотаций Big-Omega и Big-Oh.

## Describe the role of _constants and lower-order terms_ in Big-O analysis

Понимание роли констант и членов низшего порядка в анализе Big-O помогает различать производительность, которая хороша на практике, и производительность, которая хороша в теории. Оценка этих факторов приводит к наиболее точной классификации Big-O для алгоритма, предоставляя практические сведения о его эффективности.

### Constants ($c$)

Константы - это числовые множители в функциях, которые представляют точное количество операций, выполняемых алгоритмом. Однако в контексте анализа Big-O они обычно не включаются, так как не влияют на общий порядок величины.

Например, алгоритм может иметь время выполнения 7n^2. Он все равно классифицируется как O(n^2), и ведущая 7 считается незначительной в контексте асимптотического анализа. Это соответствует принципу, что для достаточно больших n умножение на константу становится менее значимым.

### Lower-Order Terms

Члены низшего порядка, также называемые "малое о", соответствуют факторам более низкого порядка в данной функции Big-O и предоставляют более детальный взгляд на поведение алгоритма.

При работе с несколькими членами:

- Доминирующий член сохраняется для представления Big-O, так как он наиболее влиятелен для больших входных данных.
- Члены низшего порядка и константы опускаются по той же причине.

Например, если алгоритм имеет сложность 3n^3 + 100n^2 + 25n, упрощение Big-O будет O(n^3), потому что член с n^3 является наиболее значимым для больших входных данных.

## Give examples of how _amortized analysis_ can provide a more balanced complexity measure

Давайте рассмотрим, как амортизированный анализ может привести к более сбалансированным мерам сложности на следующих примерах алгоритмов:

### 1. Dynamic Array List

Эта структура данных сочетает в себе преимущества быстрого произвольного доступа массива с динамическим изменением размера. Хотя отдельные вставки могут иногда вызывать дорогостоящее изменение размера, большинство вставок происходят быстрее. Через амортизированный анализ средняя стоимость операции составляет O(1).

#### Code Example: Dynamic Array List

```javascript
class DynamicArray {
  constructor() {
    this.n = 0 // Не фактический размер
    this.array = this.makeArray(1)
  }

  insert(elem) {
    if (this.n === this.array.length) {
      this._resize(2 * this.array.length)
    }
    this.array[this.n] = elem
    this.n += 1
  }

  _resize(newCapacity) {
    const newArray = this.makeArray(newCapacity)
    for (let i = 0; i < this.n; i++) {
      newArray[i] = this.array[i]
    }
    this.array = newArray
  }

  makeArray(cap) {
    return new Array(cap)
  }
}
```

### 2. Binary Search

Несмотря на его в основном логарифмическую временную сложность, существуют ситуации, когда бинарный поиск может превзойти эту эффективность через повторяющиеся операции, которые уменьшают диапазон поиска вдвое. С амортизированным анализом такие "хорошие" или "удачные" сценарии учитываются, приводя к сбалансированной временной сложности O(log n).

#### Code Example: Binary Search

```javascript
function binarySearch(arr, x) {
  let lo = 0,
    hi = arr.length - 1
  while (lo <= hi) {
    const mid = Math.floor((lo + hi) / 2)
    if (arr[mid] === x) {
      return mid
    } else if (arr[mid] < x) {
      lo = mid + 1
    } else {
      hi = mid - 1
    }
  }
  return -1
}
```

### 3. Fibonacci with Caching

Стандартные вычисления чисел Фибоначчи имеют сложность O(2^n), возникающую из-за 2^n листьев рекурсивного дерева. Однако с кэшированием одни и те же подзадачи решаются только один раз, уменьшая высоту дерева и временную сложность до линейной O(n).

#### Code Example: Caching Fibonacci

```javascript
function fibonacci(n, memo = {}) {
  if (n in memo) {
    return memo[n]
  }
  if (n <= 2) {
    return 1
  }
  memo[n] = fibonacci(n - 1, memo) + fibonacci(n - 2, memo)
  return memo[n]
}
```

## Describe how the _coefficients of higher-order terms_ affect Big-O Notation in practical scenarios

Хотя доминирующий член обычно определяет временную сложность функции, коэффициенты членов высшего порядка не обязательно незначительны. Их влияние может быть ощутимым, особенно в реальных приложениях.

### Interplay of Coefficients and Big-O

Рассмотрим сценарий, где нам нужно сравнить временные сложности на основе нотации Big-O.

1. **Низкий порядок, большой коэффициент**: 3n + 500 против 6n + 10, где n мало:

   - Big-O предполагает O(n)
   - На практике 3n имеет тенденцию превосходить 6n, особенно для малых n.

2. **Низкий порядок, малый коэффициент**: 0.05n + 3 против 0.1n + 1.5, где n велико:

   - Big-O предполагает O(n)
   - При больших n постоянные факторы все еще могут иметь заметный кумулятивный эффект.

3. **Высокий порядок, малый коэффициент**: 0.00001n^2 + 10 против 0.0001n^2 + 1:

   - Big-O предполагает O(n^2)
   - Для больших n ведущий член становится настолько доминирующим, что влияние коэффициентов становится незначительным.

4. **Высокий порядок, большой коэффициент**: 10n^2 + 5n + 2 против 2n^2 + 3n + 1 для всех n:

   - Big-O предполагает O(n^2)
   - Коэффициенты могут незначительно изменять производительность, но недостаточно, чтобы изменить асимптотическое поведение.

### Real-world Examples

Многие алгоритмы демонстрируют эти характеристики.

Алгоритмы линейного времени часто называют O(n), даже если они могут быть 4n + 2 в худшем случае. Однако для больших наборов данных разница между 4n и 0.5n может быть существенной, в отличие от теоретического постоянного фактора 4.

## Explain how _probabilistic algorithms_ can have a different Big-O Notation compared to deterministic ones

В этом сценарии давайте обсудим проблемы, для которых вероятностные алгоритмы могут предоставить приближенные решения в ожидаемом полиномиальном времени, даже если детерминированный алгоритм может потребовать экспоненциального времени.

### Problem Statement: Graph Connectivity

Цель состоит в том, чтобы выяснить, является ли граф G связным, т.е. существует ли путь между каждой парой вершин.

### Deterministic Algorithm

Стандартные алгоритмы "Поиск в ширину" или "Поиск в глубину" требуют линейного времени O(|V| + |E|) для определения связности графа.

### Probabilistic Algorithm

"Рандомизированный инкрементный алгоритм" использует другую стратегию. Он изначально считает граф несвязным, а затем добавляет ребра по одному, проверяя связность графа после каждого добавления. Этот подход в среднем работает в ожидаемом постоянном времени до того, как граф станет связным, и O(|V|), когда граф становится связным.

### Complexity Analysis

#### Deterministic Algorithm

Сложность |V| + |E| сохраняется для графов с большим ожидаемым отношением количества вершин к количеству ребер.

#### Probabilistic Algorithm

- Первый этап, когда граф несвязен, ожидает операцию постоянного времени (обычно заканчивается преждевременно).

- Второй этап, активируемый с вероятностью меньше (2/3)^(3/2n), которая может быть ограничена сверху константой c, диктует сложность O(c|V|).

Таким образом, в ожидаемом полиномиальном времени O(n^2) алгоритм сочетает предварительное постоянное время и ограниченное сверху линейное время.

## Analyze the Big-O _time complexity_ of array operations

Давайте рассмотрим общие операции с массивами и их временные сложности Big-O.

### Time Complexity in Arrays

#### Access $O(1)$

Доступ к элементу массива по его индексу - это операция с постоянным временем.

#### Search $O(n)$

- Для неотсортированного массива поиск конкретного значения может иметь временную сложность в худшем случае O(n), так как может потребоваться проверка каждого элемента для нахождения цели.

- Для отсортированного массива может быть реализован бинарный поиск, снижающий сложность до O(log n).

#### Insert $O(n)$

Вставка элемента в определенный индекс может потребовать сдвига элементов после этого индекса к следующему индексу. Этот процесс сдвига, особенно в списке массивов, является процессом линейного времени.

Добавление элемента (вставка в конец) обычно может быть выполнено за амортизированное постоянное время, если только массиву не требуется изменение размера, в этом случае это становится операцией линейного времени.

#### Delete $O(n)$

Удаление элемента по определенному индексу, вероятно, потребует сдвига последующих элементов, что является операцией линейного времени.

Если вы удаляете последний элемент, это операция с постоянным временем.

### Other Operations

- **Сортировка**:

Многие популярные алгоритмы сортировки (например, быстрая сортировка, сортировка слиянием) имеют среднюю и худшую временную сложность O(n log n).

- **Транспонирование**:

Перестановка элементов или обмен соседних элементов, например, в операции "транспонирование", является процессом с постоянным временем.

- **Слияние в отсортированном порядке**:

Если оба массива отсортированы в порядке возрастания или убывания, то для многих алгоритмов возможно "слияние" двух массивов в один отсортированный массив за время, линейное общему количеству элементов (помните, сортировка целых наборов данных обычно является операцией O(n log n)).

Помните, что эти сложности являются общими. Фактическая производительность может зависеть от оборудования и конкретного языка программирования. Зная эти сложности, вы можете лучше понимать и предсказывать эффективность вашего кода.

## Discuss the Big-O _space complexity_ of using _linked lists_

Хотя связные списки предлагают операции вставки и удаления O(1), ключевым моментом является их пространственная сложность из-за "накладных расходов памяти на узел". Каждый узел занимает память как для своих данных, так и для указателя, что делает их пространственную сложность O(n).

Это означает, что в худшем случае они могут занимать столько же места, как массив размера O(n), не предлагая преимуществ непрерывной памяти массивов. Различные типы связных списков могут влиять на пространственную сложность, так как меньшие указатели в типах вроде "Двусвязных списков" занимают больше места, а вспомогательные указатели в "Двусвязных списках с вспомогательными указателями" могут еще больше усугубить накладные расходы. Стандартом для пространства является "Односвязный список".

### Memory Overhead in Different Types of Linked Lists

- **Односвязный список**:

Каждый узел имеет один указатель, обычно 4-8 байт на 64-битной системе, в дополнение к данным. Таким образом, O(1) указателей на узел делает его O(n) с точки зрения пространства.

- **Двусвязный список**:

Каждый узел имеет два указателя, требующих либо 8-16 байт. Хотя это все еще O(1) с точки зрения указателей, он имеет сравнительно более высокие накладные расходы, чем односвязный список.

- **Циклический список**:

Аналогичен односвязным и двусвязным спискам, но хвостовой узел указывает обратно на голову. Это не влияет на пространственную сложность.

- **XOR-связный список**:

Использует побитовые операции XOR для хранения только одной ссылки (в отличие от двух в двусвязных списках). Однако он довольно сложен в реализации и поддержке, поэтому используется в основном в образовательных или теоретических целях.

- **Односвязные и двусвязные списки с вспомогательными указателями**:

Эти списки имеют дополнительный указатель, увеличивая требования к памяти на узел и, следовательно, пространственную сложность.

## Compare the Big-O complexities of various _sorting algorithms_

Давайте рассмотрим сложности Big-O различных фундаментальных алгоритмов сортировки.

### Bubble Sort

Сложность:

- Лучший случай: O(n) когда список уже отсортирован благодаря флагу swapped.
- Худший случай: O(n^2) когда каждый элемент требует n-1 сравнений, и есть n элементов.

### Selection Sort

Сложность:

- Лучший случай: O(n^2) потому что n-1 проходов все еще делаются для нахождения наибольшего элемента.
- Худший случай: O(n^2) по той же причине.

### Insertion Sort

Сложность:

- Лучший случай: O(n) когда список уже отсортирован.
- Худший случай: O(n^2) возникает, когда список отсортирован в обратном порядке. n-1 сравнений и присваиваний делаются для каждого из n элементов.

### Merge Sort

Сложность:

- Лучший случай: O(n log n) так как он разделяет список поровну и требует n log n времени для слияния.
- Худший случай: O(n log n) по тем же причинам.

### Quick Sort

Сложность:

- Лучший случай: O(n log n) когда выбранный опорный элемент всегда делит список поровну.
- Худший случай: O(n^2) когда опорный элемент всегда является одним из наименьших или наибольших элементов. Это более вероятно, если список уже частично отсортирован.

### Heap Sort

Сложность:

- Лучший случай: O(n log n) - Операция heapify занимает O(n) времени, а операции с кучей занимают O(log n) времени.

- Худший случай: O(n log n) по той же причине.

### Counting Sort

Сложность:

- Лучший случай: O(n + k) для k различных элементов с временной сложностью O(n).
- Худший случай: O(n + k) для k различных элементов.

### Radix Sort

Сложность:

- Лучший случай: O(nk), где k - количество цифр в максимальном числе. Фактическая сложность O(d(n + k)).
- Худший случай: O(nk) аналогично лучшему случаю, но варьируется в зависимости от распределения данных.

### Bucket Sort

Сложность:

- Лучший случай: O(n + k), где k - количество корзин или разделов. Может быть сделана O(n) когда k = n/2.
- Худший случай: O(n^2) когда все элементы попадают в одну корзину.

### Shell Sort

Сложность:

- Лучший случай: Зависит от последовательности промежутков, но обычно O(n log n).
- Худший случай: O(n(log n)^2) обычно, но может варьироваться в зависимости от последовательности промежутков.

## Evaluate the Big-O _time complexity_ of _binary search_

Бинарный поиск - это алгоритм типа "разделяй и властвуй", в основном используемый на отсортированных списках для эффективного нахождения элемента. Он предлагает временную сложность O(log n).

### Key Features

- **Заметная эффективность**: Бинарный поиск превосходит линейный поиск, который имеет временную сложность O(n), особенно на более длинных списках.

- **Рекурсивная или итеративная реализация**: Алгоритм может быть реализован с использованием рекурсивного или итеративного подхода.

- **Ограничение памяти**: Бинарный поиск навигирует по списку в памяти без необходимости в дополнительных структурах данных, что делает его идеальным для больших наборов данных с ограниченными ресурсами памяти.

### Time Complexity Analysis

Временная сложность бинарного поиска оценивается с использованием рекуррентного соотношения, используя преимущества Основной теоремы:

T(n) = T(n/2) + 1

- T(n) - это временная сложность.
- Постоянная работа, выполняемая внутри каждого рекурсивного вызова, равна 1.
- Алгоритм делит список на подсписки длиной n/2.

### Pseudocode

```
Пусть min = 0 и max = n-1
Пока min <= max
    Выполнить вычисление середины
    Если arr[middle] = target
        : цель найдена, вернуть middle
    Если arr[middle] < target
        : Отбросить левый подсписок (установить min = middle + 1)
    Иначе
        : Отбросить правый подсписок (установить max = middle - 1)
Вернуть "не найдено"
```

### Time Complexity Breakdown

- **Управление циклом**: Хотя это не всегда верно, основной контроль цикла обычно проверяет `min <= max`. Каждая итерация, таким образом, помогает уменьшить размер рассматриваемого подсписка вдвое.

- **Количество итераций**: Количество итераций в худшем случае обычно определяет, насколько эффективно бинарный поиск может уменьшить пространство поиска. Кроме того, общую сложность можно выразить как:

  1 + 2 + 4 + ... + 2^k

Последний член может достигать 2^k, что может не точно соответствовать, но должно быть достаточно близко для оценки общей временной сложности алгоритма.

### In-Place vs. Memoization/Tabulation

"Бинарный поиск" использует подход in-place. Он перемещается по входному списку без необходимости в дополнительных структурах данных, демонстрируя пространственную сложность O(1).

## Determine the Big-O _time and space complexities_ of _hash table_ operations

Давайте обсудим временную и пространственную сложность, связанную с основными операциями хеш-таблиц.

### Key Operations

**Поиск (Найти элемент) - O(1)**

- Хеш-функция определяет местоположение элемента в уникальной хеш-таблице.

**Вставка (Добавить новый элемент) - O(1)**

- Хеш-функция определяет размещение, обычно за постоянное время.

**Удаление (Удалить элемент) - O(1)**

- Аналогично "Вставке", этот шаг обычно требует только постоянного времени.

### Resizing Mechanism

**Изменение размера (Рехеширование для динамических таблиц) - O(n)**

- Что касается амортизированного времени, изменение размера массивов занимает линейное время, но происходит нечасто. Таким образом, среднее время амортизируется на многие операции.

**Рехеширование (во время изменения размера) - O(n)**

- Повторная вставка всех элементов происходит линейно с количеством элементов в таблице.

### Why is Search O(1)?

В идеальном случае каждый ключ отображается на уникальный хеш, и, следовательно, на уникальную ячейку таблицы. При работе с коллизиями (два ключа хешируются в одну ячейку) некоторые алгоритмы превосходят другие.

Время, необходимое для разрешения коллизии, имеет решающее значение для формирования амортизированных границ таблицы. Методы включают раздельное связывание и открытую адресацию.

### Complexity Summary

- Худший случай по времени: O(n) (Все ключи сталкиваются)
- Амортизированное время: O(1)
- Худший случай по пространству: O(n)

## Discuss the Big-O complexities of _tree_ operations, including binary search trees and AVL trees

Бинарные деревья поиска (BST) эффективны как для поиска, так и для вставки операций, со средней временной сложностью O(log n). Однако неоптимальные структуры могут привести к худшей временной сложности O(n).

### Key Characteristics

**Поиск (Лучший и худший случай): O(log n)**

- Лучший случай: Корень дерева является целью. Каждый уровень исключает половину оставшихся узлов.
- Худший случай: Дерево представляет собой один связный список.

**Вставка: O(log n)**

- Обеспечивает сбалансированность дерева.

#### Пример кода: Вставка в BST

```javascript
class Node {
  constructor(value) {
    this.left = null
    this.right = null
    this.value = value
  }
}

function insert(root, value) {
  if (root === null) {
    return new Node(value)
  }
  if (value < root.value) {
    root.left = insert(root.left, value)
  } else {
    root.right = insert(root.right, value)
  }
  return root
}
```

### AVL Trees

АВЛ-деревья максимизируют эффективность поиска, оставаясь сбалансированными. Они гарантируют худшую временную сложность O(log n) для поиска и вставки.

#### Rotations for Balancing

**Одиночное вращение**: Используется, когда дисбаланс возникает из-за операций либо в левом, либо в правом поддереве узла.

- Левое вращение: Разрешает ситуацию с перевесом влево.
- Правое вращение: Разрешает ситуацию с перевесом вправо.

**Двойное вращение**: Применяется, когда дисбаланс узла вызван операциями, исходящими из обоих поддеревьев.

- Лево-правое (или право-левое) вращение: Применяет как левое, так и правое (или наоборот) вращение для восстановления баланса.

#### Example of Rotation

Вот визуальное представление:

До вращения:

```
    10
   /  \
  5   15
```

После левого вращения:

```
    10
   /  \
  5   15
```

#### Code Example: AVL Insertion

```javascript
class Node {
  constructor(value) {
    this.left = null
    this.right = null
    this.value = value
    this.height = 1
  }
}

function insert(root, value) {
  if (root === null) {
    return new Node(value)
  }
  if (value < root.value) {
    root.left = insert(root.left, value)
  } else {
    root.right = insert(root.right, value)
  }

  root.height = 1 + Math.max(getHeight(root.left), getHeight(root.right))
  const balance = getBalance(root)

  if (balance > 1 && value < root.left.value) {
    return rightRotate(root)
  }
  if (balance < -1 && value > root.right.value) {
    return leftRotate(root)
  }
  if (balance > 1 && value > root.left.value) {
    root.left = leftRotate(root.left)
    return rightRotate(root)
  }
  if (balance < -1 && value < root.right.value) {
    root.right = rightRotate(root.right)
    return leftRotate(root)
  }

  return root
}

function getHeight(node) {
  return node ? node.height : 0
}

function getBalance(node) {
  return getHeight(node.left) - getHeight(node.right)
}

function rightRotate(z) {
  const y = z.left
  const t3 = y.right
  y.right = z
  z.left = t3
  z.height = 1 + Math.max(getHeight(z.left), getHeight(z.right))
  y.height = 1 + Math.max(getHeight(y.left), getHeight(y.right))
  return y
}

function leftRotate(z) {
  const y = z.right
  const t2 = y.left
  y.left = z
  z.right = t2
  z.height = 1 + Math.max(getHeight(z.left), getHeight(z.right))
  y.height = 1 + Math.max(getHeight(y.left), getHeight(y.right))
  return y
}
```

## Analyze the Big-O complexity of _graph_ algorithms, including traversal and shortest path algorithms

Алгоритмы для графов различаются по своим вычислительным требованиям. Временная сложность часто измеряется в нотации Big O.

Вот подробный разбор:

### Graph Traversal

**Поиск в глубину (DFS)**

- Временная сложность: O(V + E) - Посещение всех вершин и ребер один раз.
- Пространственная сложность: O(V) - Глубина базового стека.

**Поиск в ширину (BFS)**

- Временная сложность: O(V + E) - Посещение всех вершин и ребер один раз.
- Пространственная сложность: O(V) - Очередь обычно содержит все вершины.

### Shortest Path Algorithms

**Алгоритм Дейкстры**

- Временная сложность: O((V + E) log V) - Эффективен на практике для разреженных графов.
- Пространственная сложность: O(V) - Использование приоритетной очереди.

**Алгоритм Беллмана-Форда**

- Временная сложность: O(VE) - Медленнее, но надежнее, подходит для графов с отрицательными ребрами или циклами.
- Пространственная сложность: O(V) - Дерево кратчайших путей от одного источника.
- Обнаружение отрицательного цикла: O(V \* E)

**Алгоритм Флойда-Уоршелла**

- Временная сложность: O(V^3) - Находит кратчайшие пути между всеми парами.
- Пространственная сложность: O(V^2)
- Формулировка динамического программирования

**Алгоритм A**

- Временная сложность: O(log V) в среднем - Допустимые и согласованные эвристики направляют поиск. Неспособность сделать это может привести к более высокой временной сложности.

- Эффективность на практике часто зависит от хорошей эвристической функции h(n).

**Алгоритм Джонсона**

- Временная сложность: O(VE + V^2 log V) - Сочетает алгоритмы Беллмана-Форда и Дейкстры с помощью функций потенциала, что делает его особенно эффективным для разреженных графов.

## Discuss time and space complexities of various _heap operations_

Кучи - это специализированные деревья, которые позволяют быстро выполнять операции вставки, удаления и поиска минимума/максимума. Они широко используются в приоритетных очередях и алгоритмах сортировки. Кучи бывают двух видов: min-куча, где наименьший ключ находится в корне, и max-куча, которая помещает наибольший ключ в корень.

### Array Operations

Индекс: i

- Родитель: ⌊(i-1)/2⌋
- Левый потомок: 2i + 1
- Правый потомок: 2i + 2

### Operations Complexity

**Вставка: O(log n)**

- Элемент для вставки помещается на уровень листьев, а затем "просеивается вверх" (перемещается вверх).
- Время: Это включает до log n обменов для восстановления структуры кучи.
- Пространство: Операция может потребовать до O(log n) пространства из-за ее итеративной природы.

**Удаление: O(log n)**

- Корневой элемент заменяется последним узлом, а затем "просеивается вниз" (перемещается вниз).
- Время: Это включает до log n обменов для сохранения свойства кучи.
- Пространство: Требует O(1) пространства, так как выполняет обмены на месте.

**Поиск минимума/максимума: O(1)**

- Наименьший или наибольший элемент, соответственно, можно найти в корне.
- Время: Это прямая операция с постоянным временем.
- Пространство: Не использует дополнительное пространство.

**Извлечение минимума/максимума: O(log n)**

- То же, что и удаление.
- Время: Как и при удалении, процесс может занять до log n обменов.
- Пространство: Требует O(1) пространства, так как выполняет обмены на месте.

### Code Example: Heap Operations

```javascript
const minHeap = []

minHeap.push(3)
minHeap.push(1)
minHeap.sort((a, b) => a - b) // Преобразование в кучу

// Удаление/ Извлечение минимума
console.log(minHeap.shift()) // Вывод: 1

// Поиск минимума/максимума
console.log(minHeap[0]) // Вывод: 3 (единственный оставшийся элемент)

// Создание max-кучи
const maxHeap = []

// Вставка элементов в max-кучу
maxHeap.push(3)
maxHeap.push(1)
maxHeap.sort((a, b) => b - a) // Преобразование в кучу

// Удаление/ Извлечение максимума
console.log(maxHeap.shift()) // Вывод: 3
```

## Provide examples of _space-time tradeoffs_ in algorithm design

Компромиссы между временем и пространством характеризуют алгоритмы, которые могут быть оптимизированы либо по времени, либо по пространству, но обычно не по обоим параметрам одновременно.

### Examples of Space-Time Tradeoffs

**Алгоритмы сжатия данных**

- Время: Использование простого алгоритма для сжатия или распаковки может сэкономить вычислительное время.
- Пространство: Более сложные методы сжатия могут требовать дополнительного пространства, но уменьшить общий размер памяти.

**Индекс для текстовых запросов**

- Время: Более глубокий механизм индексации может ускорить поиск слов для улучшения времени поиска.
- Пространство: Он потребляет дополнительную память для хранения индекса.

**Базы данных - Постоянные индексы**

- Время: Сохранение индексов на диске вместо их частого пересоздания может улучшить время запроса.
- Пространство: Требует хранения на диске.

### Code Example: Simple vs. Huffman Compression

```javascript
// Простой алгоритм сжатия (оптимизирован по времени)
const simpleCompress = (data) => {
  return data
    .split('')
    .map((char) => {
      return char.charCodeAt(0).toString(2).padStart(8, '0')
    })
    .join('')
}

// Алгоритм сжатия Хаффмана (оптимизирован по пространству)
class MinHeap {
  constructor() {
    this.heap = []
  }

  heappush(node) {
    this.heap.push(node)
    this.heap.sort((a, b) => a[0] - b[0])
  }

  heappop() {
    return this.heap.shift()
  }

  heapify() {
    this.heap.sort((a, b) => a[0] - b[0])
  }
}

function huffmanCompress(data) {
  const freq = {}
  for (const char of data) {
    freq[char] = (freq[char] || 0) + 1
  }

  const heap = new MinHeap()
  for (const [char, f] of Object.entries(freq)) {
    heap.heappush([f, [char, '']])
  }
  heap.heapify()

  while (heap.heap.length > 1) {
    const lo = heap.heappop()
    const hi = heap.heappop()
    for (const pair of lo[1]) {
      pair[1] = '0' + pair[1]
    }
    for (const pair of hi[1]) {
      pair[1] = '1' + pair[1]
    }
    heap.heappush([lo[0] + hi[0], lo[1].concat(hi[1])])
  }

  const codes = Object.fromEntries(heap.heappop()[1])
  return [
    data
      .split('')
      .map((char) => codes[char])
      .join(''),
    codes,
  ]
}
```
